{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import snowflake.connector\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "import re\n",
    "from snowflake.connector.pandas_tools import write_pandas\n",
    "import gspread\n",
    "from gspread_dataframe import get_as_dataframe, set_with_dataframe\n",
    "from google.oauth2.service_account import Credentials\n",
    "from pydrive.auth import GoogleAuth\n",
    "from pydrive.drive import GoogleDrive\n",
    "from gspread.exceptions import APIError, WorksheetNotFound\n",
    "import time\n",
    "from config import *\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import logging\n",
    "import warnings\n",
    "\n",
    "# Suppress the specific FutureWarning\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "# Cuenta de Servicio Leo\n",
    "\n",
    "# mangold-cuenta-servicio@projecto-promos.iam.gserviceaccount.com\n",
    "\n",
    "scopes = ['https://www.googleapis.com/auth/spreadsheets',\n",
    "          'https://www.googleapis.com/auth/drive']\n",
    "\n",
    "#credentials = Credentials.from_service_account_file('C:\\\\Users\\\\leonardo.mangold\\\\PycharmProjects\\\\promos_inteligencia_negocio\\\\ft_promos_automatico\\\\leo_usuario_servicio_credenciales.json', scopes=scopes)\n",
    "credentials = Credentials.from_service_account_file(jsons['credentials_mail_servicio'], scopes=scopes)\n",
    "\n",
    "gc = gspread.authorize(credentials)\n",
    "\n",
    "gauth = GoogleAuth()\n",
    "drive = GoogleDrive(gauth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intento 1\n",
      "Correct Password - connected to SNOWFLAKE\n"
     ]
    }
   ],
   "source": [
    "counter = 0\n",
    "while True:\n",
    "    print(f\"Intento {counter + 1}\")\n",
    "    if counter < 4:\n",
    "        try:\n",
    "            with open('credentials.json') as f:\n",
    "                data_pass = json.load(f)\n",
    "\n",
    "            pass_ = input(\"INGRESAR PASSCODE:\")\n",
    "\n",
    "            snowflake_connection = snowflake.connector.connect(\n",
    "                user=data_pass['snow']['USER'],\n",
    "                password=data_pass['snow']['PASS'],\n",
    "                account=data_pass['snow']['ACCOUNT'],\n",
    "                passcode=pass_,\n",
    "                database='SANDBOX_PLUS',\n",
    "                schema='DWH'\n",
    "            )\n",
    "\n",
    "            cursor = snowflake_connection.cursor()\n",
    "\n",
    "            print('Correct Password - connected to SNOWFLAKE')\n",
    "\n",
    "            break\n",
    "\n",
    "        except FileNotFoundError:\n",
    "            print(\"Error: 'credentials.json' file not found.\")\n",
    "            break\n",
    "        except json.JSONDecodeError:\n",
    "            print(\"Error: 'credentials.json' file is not valid JSON.\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            counter += 1\n",
    "            print(f'Error: {e}')\n",
    "            print('Incorrect Password - provide again')\n",
    "\n",
    "    else:\n",
    "        print('3 Intentos fallidos')\n",
    "        break"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# 2501\tFrescos Marzo 2025\n",
    "# 2502\tFrescos Abril 2025"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "with open('URLS.txt', 'r') as file:\n",
    "# Skip the first row\n",
    "    next(file)\n",
    "\n",
    "    # Iterate over the remaining rows\n",
    "    for row in file:\n",
    "\n",
    "        # Remove any leading/trailing whitespace characters\n",
    "        url = row.strip()\n",
    "        codigo_url = url.split('/')[-2]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "05. OFERTATA ENERO 04 (2412) ARMADO\n"
     ]
    }
   ],
   "source": [
    "gs = gc.open_by_key(codigo_url)\n",
    "\n",
    "try:\n",
    "    worksheetL = gs.worksheet('Listado')\n",
    "except gspread.exceptions.WorksheetNotFound:\n",
    "    try:\n",
    "        worksheetL = gs.worksheet('ARMADO')\n",
    "    except gspread.exceptions.WorksheetNotFound:\n",
    "        print(f\"{gs.title} - Listado and Armado worksheets not found\")\n",
    "print(gs.title, worksheetL.title)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Inicio"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "# Parte 1. Defino la funcion\n",
    "# Inputs: url y worksheet\n",
    "\n",
    "\n",
    "def retry_api_call(function, retries=5, initial_delay=1, log=True):\n",
    "    \"\"\"\n",
    "    Retries the provided function if an APIError occurs, with exponential backoff.\n",
    "    \"\"\"\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            return function()\n",
    "        except APIError as e:\n",
    "            error_code = getattr(e.response, 'status_code', None)\n",
    "            if error_code in {503, 500}:\n",
    "                if log:\n",
    "                    logging.warning(f\"API error {error_code}. Retrying in {initial_delay * (2 ** attempt)} seconds... (Attempt {attempt + 1}/{retries})\")\n",
    "                time.sleep(initial_delay * (2 ** attempt))  # Exponential backoff\n",
    "            else:\n",
    "                raise e\n",
    "    if log:\n",
    "        logging.error(\"API is still unavailable after multiple retries.\")\n",
    "    raise APIError(\"Failed after multiple retries. The service may be down.\")\n",
    "\n",
    "\n",
    "def descargar_promos_url(cursor, url, worksheet):\n",
    "\n",
    "    codigo_url = url.split('/')[-2]\n",
    "    gs = gc.open_by_key(codigo_url)\n",
    "\n",
    "    try:\n",
    "        worksheetL = worksheet\n",
    "    except gspread.exceptions.WorksheetNotFound:\n",
    "        print(f\"{worksheet} not found\")\n",
    "\n",
    "    evento_id = gs.title.split('(')[-1].split(')')[0]\n",
    "    if not (evento_id.isdigit() and len(evento_id) ==4):\n",
    "        print(f'Error en tomar evento_id: {gs.title}')\n",
    "        sys.exit()\n",
    "\n",
    "    time.sleep(5)\n",
    "\n",
    "    data = worksheetL.get_all_values()\n",
    "    juli0 = pd.DataFrame(data)\n",
    "\n",
    "    #\n",
    "    # if worksheetL:\n",
    "    #     try:\n",
    "    #         data = worksheetL.get_all_values()\n",
    "    #\n",
    "    #         juli0 = pd.DataFrame(data)\n",
    "    #\n",
    "    #\n",
    "    #     except:\n",
    "    #         #data = worksheetL.get_all_values(value_render_option='FORMATTED_VALUE')\n",
    "    #         data = worksheetL.get_all_values()\n",
    "    #         juli0 = pd.DataFrame(data)\n",
    "\n",
    "    evento_comercial_position = None\n",
    "    for i, row in juli0.iterrows():\n",
    "        for j, value in row.items():\n",
    "            if value == 'EVENTO COMERCIAL':\n",
    "                evento_comercial_position = (i, j)\n",
    "                break\n",
    "        if evento_comercial_position:\n",
    "            break\n",
    "\n",
    "    evento_comercial_position\n",
    "    juli0.iloc[evento_comercial_position]\n",
    "\n",
    "    add_nombre_evento = (0, 2)\n",
    "    nombre_evento_position = (evento_comercial_position[0] + add_nombre_evento[0], evento_comercial_position[1] + add_nombre_evento[1])\n",
    "    nombre_evento = juli0.iloc[nombre_evento_position]\n",
    "\n",
    "    add_fecha_ini = (1, 2)\n",
    "    fecha_ini_position = (evento_comercial_position[0] + add_fecha_ini[0], evento_comercial_position[1] + add_fecha_ini[1])\n",
    "    fini = juli0.iloc[fecha_ini_position]\n",
    "\n",
    "    add_fecha_fin = (2, 2)\n",
    "    fecha_fin_position = (evento_comercial_position[0] + add_fecha_fin[0], evento_comercial_position[1] + add_fecha_fin[1])\n",
    "    ffin = juli0.iloc[fecha_fin_position]\n",
    "\n",
    "    add_estado_articulos = (3, 2)\n",
    "    estado_articulos_position = (evento_comercial_position[0] + add_estado_articulos[0], evento_comercial_position[1] + add_estado_articulos[1])\n",
    "    estado_articulos = juli0.iloc[estado_articulos_position]\n",
    "\n",
    "    try:\n",
    "        estado_articulos = str(estado_articulos)  # Convert to string\n",
    "        # Split by comma or period and map the result to integers\n",
    "        split_values = tuple(map(int, re.split(r'[,.]', estado_articulos)))\n",
    "\n",
    "        # If the result has only one element, return it as a single integer\n",
    "        if len(split_values) == 1:\n",
    "            #estado_articulos = split_values[0]\n",
    "            estado_articulos = '(' + str(split_values[0]) + ')'\n",
    "        else:\n",
    "            estado_articulos = split_values\n",
    "\n",
    "        print(f\"Estados: {estado_articulos}\")\n",
    "    except ValueError as e:\n",
    "        print(f\"{gs.title} - Error: {e}. Could not convert one or more items to integers.\")\n",
    "        return\n",
    "\n",
    "    add_clave_estado_articulos = (3, 0)\n",
    "    estado_clave_estado_articulos = (evento_comercial_position[0] + add_clave_estado_articulos[0], evento_comercial_position[1] + add_clave_estado_articulos[1])\n",
    "    clave_estado_articulos = juli0.iloc[estado_clave_estado_articulos]\n",
    "\n",
    "    # ESTADISTICO / ORIN / DESCRIPCION / PRECIO PLUS / ESTIMADO CERCANIAS / GO\n",
    "\n",
    "    juli = pd.DataFrame()\n",
    "\n",
    "    estadistico_position = None\n",
    "    for i, row in juli0.iterrows():\n",
    "        for j, value in row.items():\n",
    "            if value == 'ESTADÍSTICO':\n",
    "                estadistico_position = (i, j)\n",
    "                break\n",
    "        if estadistico_position:\n",
    "            break\n",
    "\n",
    "    if estadistico_position:\n",
    "        start_row, start_col = estadistico_position\n",
    "        juli = juli0.iloc[start_row:, juli0.columns.get_loc(start_col):]\n",
    "        juli.columns = juli.iloc[0]\n",
    "        juli = juli.drop(juli.index[0])\n",
    "    else:\n",
    "        print(f\"{gs.title} - Value 'ESTADÍSTICO' not found in the DataFrame.\")\n",
    "\n",
    "    columns_to_consider = [0, 1, 2, 4]\n",
    "    selected_columns = juli.columns[columns_to_consider].tolist()\n",
    "\n",
    "    if 'ESTIMADO CERCANIAS' in juli.columns:\n",
    "        selected_columns.append('ESTIMADO CERCANIAS')\n",
    "\n",
    "    # GO\n",
    "\n",
    "    try:\n",
    "        go_sheet = gs.worksheet('Guia Op.')\n",
    "    except gspread.exceptions.WorksheetNotFound:\n",
    "        print(f\"{gs.title} - Guia Op. worksheet not found\")\n",
    "\n",
    "    data = go_sheet.get_all_values()\n",
    "    go_df = pd.DataFrame(data)\n",
    "    go_df = go_df[[3]]\n",
    "    go_df.rename({3:'GO'}, axis=1, inplace=True)\n",
    "    go_df = go_df.dropna()\n",
    "    go_df = go_df[go_df['GO'] != '']\n",
    "\n",
    "    ## Comienza flujo\n",
    "\n",
    "    df = juli.copy()\n",
    "\n",
    "    if 'GO' not in df.columns:\n",
    "        df['GO'] = np.nan\n",
    "\n",
    "    df.loc[df['ORIN'].isin(go_df['GO']), 'GO'] = 1\n",
    "\n",
    "    hoy = datetime.today().date().strftime('%Y-%m-%d')\n",
    "\n",
    "    try:\n",
    "        df.to_csv(f\"{paths['respaldo_BI']}\\\\Drive Lu - {evento_id} - {hoy}.csv\", index=False)\n",
    "    except:\n",
    "        df.to_csv(f\"Drive Lu - {evento_id} - {hoy}.csv\", index=False) # Si no puede guardar en la carpeta compartida, guarda en nuestro directorio local\n",
    "        print(f\"{gs.title} -Guardado en PC\")\n",
    "\n",
    "    #### LIMPIO ESTADISTICOS NO NUMERICOS\n",
    "    df['ESTADÍSTICO'] = pd.to_numeric(df['ESTADÍSTICO'], errors='coerce')\n",
    "    df = df.dropna(subset=['ESTADÍSTICO'])\n",
    "\n",
    "    if df.empty:\n",
    "        print(f\"{gs.title} - No valid 'ESTADÍSTICO' data after cleaning, skipping.\")\n",
    "        #continue  # Skip to the next URL if no valid data\n",
    "        return\n",
    "\n",
    "    df['ESTADÍSTICO']=df['ESTADÍSTICO'].astype(int)\n",
    "    df.rename(columns={\n",
    "        \"ESTADÍSTICO\": \"ESTADISTICO\",\n",
    "        'PRECIO PLUS':'PVP OFERTA',\n",
    "        'DESCRIPCIÓN':'DESCRIPCION',\n",
    "        'GO':'ESTIBA_GUIA_OPERATIVA'\n",
    "    }, inplace=True)\n",
    "\n",
    "    ### GENERO LISTA DE ESTADISTICOS Y ORINES PARA BUSCAR EN SNOW\n",
    "    df['ESTADISTICO'] = df['ESTADISTICO'].apply(lambda x: str(x))\n",
    "    df['ESTADISTICO']=df['ESTADISTICO'].str.strip()\n",
    "    estadistico = df['ESTADISTICO'].tolist()\n",
    "    estadistico = ', '.join(f\"'{item}'\" for item in estadistico)\n",
    "    df['ORIN'] = df['ORIN'].apply(lambda x: str(x))\n",
    "    df['ORIN']=df['ORIN'].str.strip()\n",
    "    df['ORIN'].replace('nan', pd.NA, inplace=True)\n",
    "    df_orin=df[['ORIN']].copy()\n",
    "    df_orin=df_orin.drop_duplicates()\n",
    "    df_orin= df_orin.dropna(subset=['ORIN'])\n",
    "    ORIN = df_orin['ORIN'].tolist()\n",
    "    ORIN = ', '.join(f\"'{item}'\" for item in ORIN)\n",
    "    duplis_drive=df.groupby(['ESTADISTICO','ORIN'])['DESCRIPCION'].count().reset_index().sort_values(by='DESCRIPCION', ascending=False)\n",
    "    df_t=df.sort_values(by=['ESTADISTICO','ORIN','PVP OFERTA'])\n",
    "    df_no_duplicates = df_t.drop_duplicates(subset=['ESTADISTICO','ORIN'], keep='first')\n",
    "\n",
    "    ### Locales\n",
    "\n",
    "    cercania = pd.DataFrame()\n",
    "\n",
    "    if 'ESTIMADO CERCANIAS' in df.columns:\n",
    "        cercania = df[~df['ESTIMADO CERCANIAS'].isna()]\n",
    "        cercania = df[df['ESTIMADO CERCANIAS'] != '']\n",
    "\n",
    "    locales_mailing = 101,102,103,104,107,108,111,115,117,119,120,121,122,124,125,126,127,131,132,134,139,140,141,143,144,145,146,147,148,149,150,151,152,153,155,156,157,158,159,160,161,162,163,164,165,167,166,168,173,198,301,304,305,306,307,308,309,310,311,312,313,314,315,316,317,318,319,321,322,323,324,325,326,327,328,329,330,331,332,333,334,335,417,480,482,483,484,485,486,487,488,489,490,491,492,493,494,495,496,497\n",
    "\n",
    "    #len(locales_mailing) # supers + hipers + 2 express\n",
    "\n",
    "    locales_cercania = 302, 320, 401, 403, 410, 411, 412, 418, 419, 405, 406\n",
    "    #len(locales_cercania)\n",
    "\n",
    "    if len([value for value in locales_cercania if value in locales_mailing]) != 0:\n",
    "        print(f\"{gs.title} - Error - se repiten locales\")\n",
    "        sys.exit()\n",
    "\n",
    "    # 1. LOCALES MAILING\n",
    "\n",
    "    if clave_estado_articulos.lower() == 'estado artículos': # me especifican estado articulo\n",
    "        #print('entra en Estado Artículos')\n",
    "        query = f'''\n",
    "                SELECT\n",
    "                    DISTINCT\n",
    "                    '{fini}' AS \"Fecha Desde\",\n",
    "                    '{ffin}' AS \"Fecha Hasta\",\n",
    "                    LAA.ARTC_ARTC_COD AS ESTADISTICO,\n",
    "                    LAA.ORIN,\n",
    "                    {evento_id} AS \"Nombre Evento\",\n",
    "                    NULL AS \"Pronostico de Venta\",\n",
    "                    0 AS \"Stock Inicial Promo\",\n",
    "                    LGL.GEOG_LOCL_COD AS LOCAL\n",
    "                FROM\n",
    "                    MSTRDB.DWH.FT_STOCK AS FS\n",
    "                    INNER JOIN MSTRDB.DWH.LU_ARTC_ARTICULO AS LAA ON FS.ARTC_ARTC_ID = LAA.ARTC_ARTC_ID\n",
    "                    INNER JOIN MSTRDB.DWH.LU_GEOG_LOCAL AS LGL ON\n",
    "                        FS.GEOG_LOCL_ID = LGL.GEOG_LOCL_ID\n",
    "                        AND LGL.GEOG_LOCL_COD IN {locales_mailing}\n",
    "                WHERE\n",
    "                    FS.TIEM_DIA_ID = CURRENT_DATE - 1\n",
    "                    AND (LAA.ORIN IN ({ORIN}) OR LAA.ARTC_ARTC_COD IN ({estadistico}))\n",
    "                    AND LAA.ORIN <> -1\n",
    "                    AND FS.ARTC_ESTA_ID IN {estado_articulos}\n",
    "                    '''\n",
    "\n",
    "    else:\n",
    "        #print('NO entra en Estado Artículos')\n",
    "        query = f'''\n",
    "                SELECT\n",
    "                    DISTINCT\n",
    "                    '{fini}' AS \"Fecha Desde\",\n",
    "                    '{ffin}' AS \"Fecha Hasta\",\n",
    "                    LAA.ARTC_ARTC_COD AS ESTADISTICO,\n",
    "                    LAA.ORIN,\n",
    "                    {evento_id} AS \"Nombre Evento\",\n",
    "                    NULL AS \"Pronostico de Venta\",\n",
    "                    0 AS \"Stock Inicial Promo\",\n",
    "                    LGL.GEOG_LOCL_COD AS LOCAL\n",
    "                FROM\n",
    "                    MSTRDB.DWH.FT_STOCK AS FS\n",
    "                    INNER JOIN MSTRDB.DWH.LU_ARTC_ARTICULO AS LAA ON FS.ARTC_ARTC_ID = LAA.ARTC_ARTC_ID\n",
    "                    INNER JOIN MSTRDB.DWH.LU_GEOG_LOCAL AS LGL ON\n",
    "                        FS.GEOG_LOCL_ID = LGL.GEOG_LOCL_ID\n",
    "                        AND LGL.GEOG_LOCL_COD IN {locales_mailing}\n",
    "                WHERE\n",
    "                    FS.TIEM_DIA_ID = CURRENT_DATE - 1\n",
    "                    AND (LAA.ORIN IN ({ORIN}) OR LAA.ARTC_ARTC_COD IN ({estadistico}))\n",
    "                    AND LAA.ORIN <> -1\n",
    "                    AND FS.ARTC_ESTA_ID IN (4, 6)\n",
    "                    '''\n",
    "\n",
    "    cursor.execute(query)\n",
    "    info = cursor.fetch_pandas_all()\n",
    "\n",
    "    # 2. LOCALES CERCANIA\n",
    "\n",
    "    info_cercania = pd.DataFrame()\n",
    "\n",
    "    if len(cercania) > 0:\n",
    "\n",
    "        ORIN_cercania = cercania['ORIN'].unique().tolist()\n",
    "        ORIN_cercania = ', '.join(f\"'{item}'\" for item in ORIN_cercania)\n",
    "\n",
    "        estadistico_cercania = cercania['ESTADISTICO'].unique().tolist()\n",
    "        estadistico_cercania = ', '.join(f\"'{item}'\" for item in estadistico_cercania)\n",
    "\n",
    "        if clave_estado_articulos.lower() == 'estado artículos': # me especifican estado articulo\n",
    "            query = f'''\n",
    "                    SELECT\n",
    "                        DISTINCT\n",
    "                        '{fini}' AS \"Fecha Desde\",\n",
    "                        '{ffin}' AS \"Fecha Hasta\",\n",
    "                        LAA.ARTC_ARTC_COD AS ESTADISTICO,\n",
    "                        LAA.ORIN,\n",
    "                        {evento_id} AS \"Nombre Evento\",\n",
    "                        NULL AS \"Pronostico de Venta\",\n",
    "                        0 AS \"Stock Inicial Promo\",\n",
    "                        LGL.GEOG_LOCL_COD AS LOCAL\n",
    "                    FROM\n",
    "                        MSTRDB.DWH.FT_STOCK AS FS\n",
    "                        INNER JOIN MSTRDB.DWH.LU_ARTC_ARTICULO AS LAA ON FS.ARTC_ARTC_ID = LAA.ARTC_ARTC_ID\n",
    "                        INNER JOIN MSTRDB.DWH.LU_GEOG_LOCAL AS LGL ON\n",
    "                            FS.GEOG_LOCL_ID = LGL.GEOG_LOCL_ID\n",
    "                            AND LGL.GEOG_LOCL_COD IN {locales_cercania}\n",
    "                    WHERE\n",
    "                        FS.TIEM_DIA_ID = CURRENT_DATE - 1\n",
    "                        AND (LAA.ORIN IN ({ORIN_cercania}) OR LAA.ARTC_ARTC_COD IN ({estadistico_cercania}))\n",
    "                        AND LAA.ORIN <> -1\n",
    "                        AND FS.ARTC_ESTA_ID IN {estado_articulos}\n",
    "                    '''\n",
    "\n",
    "        else:\n",
    "            query = f'''\n",
    "                    SELECT\n",
    "                        DISTINCT\n",
    "                        '{fini}' AS \"Fecha Desde\",\n",
    "                        '{ffin}' AS \"Fecha Hasta\",\n",
    "                        LAA.ARTC_ARTC_COD AS ESTADISTICO,\n",
    "                        LAA.ORIN,\n",
    "                        {evento_id} AS \"Nombre Evento\",\n",
    "                        NULL AS \"Pronostico de Venta\",\n",
    "                        0 AS \"Stock Inicial Promo\",\n",
    "                        LGL.GEOG_LOCL_COD AS LOCAL\n",
    "                    FROM\n",
    "                        MSTRDB.DWH.FT_STOCK AS FS\n",
    "                        INNER JOIN MSTRDB.DWH.LU_ARTC_ARTICULO AS LAA ON FS.ARTC_ARTC_ID = LAA.ARTC_ARTC_ID\n",
    "                        INNER JOIN MSTRDB.DWH.LU_GEOG_LOCAL AS LGL ON\n",
    "                            FS.GEOG_LOCL_ID = LGL.GEOG_LOCL_ID\n",
    "                            AND LGL.GEOG_LOCL_COD IN {locales_cercania}\n",
    "                    WHERE\n",
    "                        FS.TIEM_DIA_ID = CURRENT_DATE - 1\n",
    "                        AND (LAA.ORIN IN ({ORIN_cercania}) OR LAA.ARTC_ARTC_COD IN ({estadistico_cercania}))\n",
    "                        AND LAA.ORIN <> -1\n",
    "                        AND FS.ARTC_ESTA_ID IN (4, 6)\n",
    "                    '''\n",
    "\n",
    "        cursor.execute(query)\n",
    "        info_cercania = cursor.fetch_pandas_all()\n",
    "\n",
    "    info = pd.concat([info, info_cercania])\n",
    "\n",
    "    ### check cantidad de articulos\n",
    "\n",
    "\n",
    "    ## REVISO FALTANTES\n",
    "\n",
    "    faltantes=df_no_duplicates.copy()\n",
    "    info_unicos=info[['ESTADISTICO','ORIN','Nombre Evento']].drop_duplicates()\n",
    "\n",
    "    faltantes=faltantes.merge(info_unicos[['ESTADISTICO','Nombre Evento']], on=['ESTADISTICO'], how='left')\n",
    "    faltantes=faltantes.merge(info_unicos[['ORIN','Nombre Evento']], on=['ORIN'], how='left')\n",
    "\n",
    "    faltantes['result'] = np.where(faltantes['Nombre Evento_x'].isna() & faltantes['Nombre Evento_y'].isna(), \"NULOS\", \"Not NULOS\")\n",
    "    faltantes_final=faltantes[faltantes.result=='NULOS'].reset_index()\n",
    "    faltantes_final.drop(columns={'index'}, inplace=True)\n",
    "\n",
    "    faltantes_final['evento_id']=evento_id\n",
    "    faltantes_final['Nombre Evento']=nombre_evento\n",
    "    faltantes_final['Fecha Inicio']=fini\n",
    "    faltantes_final['Fecha Fin']=ffin\n",
    "    faltantes_final['fecha_carga'] = datetime.now()\n",
    "\n",
    "    faltantes_final=faltantes_final[['ESTADISTICO', 'ORIN', 'DESCRIPCION', 'PVP OFERTA',  'evento_id', 'Nombre Evento',\n",
    "                                     'Fecha Inicio', 'Fecha Fin', 'fecha_carga']]\n",
    "\n",
    "    hoy = datetime.now().strftime('%Y-%m-%d').replace(\"-\",\"_\")\n",
    "\n",
    "    # if len(faltantes_final)>0:\n",
    "    #\n",
    "    #     try:\n",
    "    #         gs.worksheet('FALTANTES')\n",
    "    #         worksheet1 = gs.worksheet('FALTANTES')\n",
    "    #         worksheet1.clear()\n",
    "    #         set_with_dataframe(worksheet=worksheet1, dataframe=faltantes_final, include_index=False,include_column_header=True, resize=True)\n",
    "    #\n",
    "    #     except:\n",
    "    #         new_worksheet = gs.add_worksheet(title=\"FALTANTES\", rows=faltantes_final.shape[0], cols=faltantes_final.shape[1])\n",
    "    #         worksheet1 = gs.worksheet('FALTANTES')\n",
    "    #         set_with_dataframe(worksheet=worksheet1, dataframe=faltantes_final, include_index=False,include_column_header=True, resize=True)\n",
    "    #\n",
    "    # if len(faltantes_final) == 0:\n",
    "    #     try:\n",
    "    #         gs.worksheet('FALTANTES')\n",
    "    #         worksheet1 = gs.worksheet('FALTANTES')\n",
    "    #         worksheet1.clear()\n",
    "    #\n",
    "    #     except:\n",
    "    #         pass\n",
    "\n",
    "    # LEO\n",
    "\n",
    "    if len(faltantes_final) > 0:\n",
    "        try:\n",
    "            worksheet1 = retry_api_call(lambda: gs.worksheet('FALTANTES'))\n",
    "            worksheet1.clear()\n",
    "        except WorksheetNotFound:\n",
    "            # Only create a new worksheet if faltantes_final has valid dimensions\n",
    "            if faltantes_final.shape[0] > 0 and faltantes_final.shape[1] > 0:\n",
    "                worksheet1 = retry_api_call(\n",
    "                    lambda: gs.add_worksheet(\n",
    "                        title=\"FALTANTES\", rows=faltantes_final.shape[0], cols=faltantes_final.shape[1]\n",
    "                    )\n",
    "                )\n",
    "            else:\n",
    "                raise ValueError(\"faltantes_final has invalid dimensions for worksheet creation.\")\n",
    "\n",
    "    faltantes_final['ORIN']=faltantes_final['ORIN'].str.strip()\n",
    "    faltantes_final['ORIN']= faltantes_final['ORIN'].apply(lambda x: str(x))\n",
    "    faltantes_orin_list = faltantes_final['ORIN'].tolist()\n",
    "    faltantes_orin = ', '.join(f\"'{item}'\" for item in faltantes_orin_list)\n",
    "\n",
    "    faltantes_final['ESTADISTICO'] = faltantes_final['ESTADISTICO'].str.strip()\n",
    "    faltantes_final['ESTADISTICO'] = faltantes_final['ESTADISTICO'].apply(lambda x: str(x))\n",
    "    faltantes_EST_list = faltantes_final['ESTADISTICO'].tolist()\n",
    "    faltantes_EST = ', '.join(f\"'{item}'\" for item in faltantes_EST_list)\n",
    "\n",
    "    if clave_estado_articulos == 'Estado Artículos':\n",
    "\n",
    "        query = f'''\n",
    "                select\n",
    "                    distinct artc_artc_cod,\n",
    "                    artc_artc_desc,\n",
    "                    orin,\n",
    "                    artc_esta_id\n",
    "                from\n",
    "                    mstrdb.dwh.ft_stock a\n",
    "                    inner join mstrdb.dwh.lu_artc_articulo b on a.artc_artc_id=b.artc_artc_id\n",
    "                    inner join mstrdb.dwh.lu_geog_local d on d.geog_locl_id=a.geog_locl_id and geog_locl_cod not in (100,199)\n",
    "                    where\n",
    "                        (orin in ({faltantes_orin}) or artc_artc_cod in ({faltantes_EST}))\n",
    "                        and orin<>'-1'\n",
    "                        and tiem_Dia_id=current_date()-1\n",
    "                        and geog_locl_cod in {locales_mailing}\n",
    "                        AND A.ARTC_ESTA_ID IN {estado_articulos}\n",
    "                '''\n",
    "    else:\n",
    "\n",
    "        query = f'''\n",
    "                select\n",
    "                    distinct artc_artc_cod,\n",
    "                    artc_artc_desc,\n",
    "                    orin,\n",
    "                    artc_esta_id\n",
    "                from\n",
    "                    mstrdb.dwh.ft_stock a\n",
    "                    inner join mstrdb.dwh.lu_artc_articulo b on a.artc_artc_id=b.artc_artc_id\n",
    "                    inner join mstrdb.dwh.lu_geog_local d on d.geog_locl_id=a.geog_locl_id and geog_locl_cod not in (100,199)\n",
    "                    where\n",
    "                        (orin in ({faltantes_orin}) or artc_artc_cod in ({faltantes_EST}))\n",
    "                        and orin<>'-1'\n",
    "                        and tiem_Dia_id=current_date()-1\n",
    "                        and geog_locl_cod in {locales_mailing}\n",
    "                        and artc_esta_id in (4,6)\n",
    "                '''\n",
    "\n",
    "    if len(faltantes_final)>0:\n",
    "        cursor.execute(query)\n",
    "        check_activos = cursor.fetch_pandas_all()\n",
    "\n",
    "    else:\n",
    "        check_activos = pd.DataFrame()\n",
    "\n",
    "    if clave_estado_articulos == 'Estado Artículos':\n",
    "\n",
    "        query = f'''\n",
    "                select\n",
    "                    distinct artc_artc_cod,\n",
    "                    artc_artc_desc,\n",
    "                    orin,\n",
    "                    artc_esta_id ESTADO,\n",
    "                    count(distinct a.geog_locl_id) locales\n",
    "                from\n",
    "                    mstrdb.dwh.ft_stock a\n",
    "                    inner join mstrdb.dwh.lu_artc_articulo b on a.artc_artc_id=b.artc_artc_id\n",
    "                    inner join mstrdb.dwh.lu_geog_local d on d.geog_locl_id=a.geog_locl_id and d.geog_locl_cod not in (100,199)\n",
    "                where\n",
    "                    (orin in ({faltantes_orin}) or artc_artc_cod in ({faltantes_EST}))\n",
    "                    and orin<>'-1'\n",
    "                    and tiem_Dia_id=current_date()-1\n",
    "                    and geog_locl_cod in {locales_mailing}\n",
    "                    AND A.ARTC_ESTA_ID IN {estado_articulos}\n",
    "                group by\n",
    "                    all\n",
    "                '''\n",
    "    else:\n",
    "\n",
    "        query = f'''\n",
    "                select\n",
    "                    distinct artc_artc_cod,\n",
    "                    artc_artc_desc,\n",
    "                    orin,\n",
    "                    artc_esta_id ESTADO,\n",
    "                    count(distinct a.geog_locl_id) locales\n",
    "                from\n",
    "                    mstrdb.dwh.ft_stock a\n",
    "                    inner join mstrdb.dwh.lu_artc_articulo b on a.artc_artc_id=b.artc_artc_id\n",
    "                    inner join mstrdb.dwh.lu_geog_local d on d.geog_locl_id=a.geog_locl_id and d.geog_locl_cod not in (100,199)\n",
    "                where\n",
    "                    (orin in ({faltantes_orin}) or artc_artc_cod in ({faltantes_EST}))\n",
    "                    and orin<>'-1'\n",
    "                    and tiem_Dia_id=current_date()-1\n",
    "                    and geog_locl_cod in {locales_mailing}\n",
    "                    and artc_esta_id in (4,6)\n",
    "                group by\n",
    "                    all\n",
    "                '''\n",
    "\n",
    "    if len(faltantes_final)>0:\n",
    "        cursor.execute(query)\n",
    "        check_activos = cursor.fetch_pandas_all()\n",
    "\n",
    "        check_activos.sort_values(by='ARTC_ARTC_COD')\n",
    "\n",
    "    if len(faltantes_final)>0:\n",
    "        cursor.execute(query)\n",
    "        check_activos = cursor.fetch_pandas_all()\n",
    "\n",
    "    else:\n",
    "        check_activos = pd.DataFrame()\n",
    "\n",
    "    if len(check_activos)>0:\n",
    "        try:\n",
    "            gs.worksheet('FALTANTES x ESTADO')\n",
    "            worksheet1 = gs.worksheet('FALTANTES x ESTADO')\n",
    "            worksheet1.clear()\n",
    "            set_with_dataframe(worksheet=worksheet1, dataframe=check_activos, include_index=False,include_column_header=True, resize=True)\n",
    "\n",
    "        except:\n",
    "            new_worksheet = gs.add_worksheet(title=\"FALTANTES x ESTADO\", rows=check_activos.shape[0], cols=check_activos.shape[1])\n",
    "            worksheet1 = gs.worksheet('FALTANTES x ESTADO')\n",
    "            set_with_dataframe(worksheet=worksheet1, dataframe=check_activos, include_index=False,include_column_header=True, resize=True)\n",
    "\n",
    "    if len(check_activos) == 0:\n",
    "\n",
    "        try:\n",
    "            gs.worksheet('FALTANTES x ESTADO')\n",
    "            worksheet1 = gs.worksheet('FALTANTES x ESTADO')\n",
    "            worksheet1.clear()\n",
    "\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    query=f\"\"\"select distinct artc_artc_cod,artc_artc_desc, orin, artc_artc_id, ARTC_ARTC_FFIN\n",
    "            from  mstrdb.dwh.lu_artc_articulo b where (orin in ({faltantes_orin}) or artc_artc_cod in ({faltantes_EST}))\n",
    "            and orin<>'-1' \"\"\"\n",
    "\n",
    "    if len(faltantes_final)>0:\n",
    "        cursor.execute(query)\n",
    "        check_articulos = cursor.fetch_pandas_all()\n",
    "\n",
    "    if len(faltantes_final)>0:\n",
    "        faltantes_en_lu_articulo0 = pd.DataFrame({'ORIN': faltantes_orin_list},index=range(len(faltantes_orin_list)))\n",
    "\n",
    "\n",
    "        faltantes_en_lu_articulo = faltantes_en_lu_articulo0[~faltantes_en_lu_articulo0['ORIN'].isin(check_articulos['ORIN'])]\n",
    "\n",
    "\n",
    "    if len(faltantes_final) > 0:\n",
    "        if len(faltantes_en_lu_articulo) > 0:\n",
    "            try:\n",
    "                # Try to get the existing worksheet\n",
    "                worksheet1 = retry_api_call(lambda: gs.worksheet('FALTANTES en MAESTRO ARTC'))\n",
    "                worksheet1.clear()  # Clear the existing worksheet\n",
    "            except WorksheetNotFound:\n",
    "                # Create a new worksheet only if faltantes_final has valid dimensions\n",
    "                if faltantes_final.shape[0] > 0 and faltantes_final.shape[1] > 0:\n",
    "                    worksheet1 = retry_api_call(\n",
    "                        lambda: gs.add_worksheet(\n",
    "                            title=\"FALTANTES en MAESTRO ARTC\",\n",
    "                            rows=faltantes_final.shape[0],\n",
    "                            cols=faltantes_final.shape[1]\n",
    "                        )\n",
    "                    )\n",
    "                else:\n",
    "                    raise ValueError(\"faltantes_final has invalid dimensions for worksheet creation.\")\n",
    "\n",
    "            # Insert values after clearing or creating the worksheet\n",
    "            set_with_dataframe(\n",
    "                worksheet=worksheet1,\n",
    "                dataframe=df[df.ORIN.isin(faltantes_en_lu_articulo.ORIN)],\n",
    "                include_index=False,  # Don't include index\n",
    "                include_column_header=True,  # Include column headers\n",
    "                resize=True  # Resize columns to fit content\n",
    "            )\n",
    "\n",
    "    # LEO\n",
    "\n",
    "    # if len(faltantes_final) > 0:\n",
    "    #     if len(faltantes_en_lu_articulo) > 0:\n",
    "    #         try:\n",
    "    #             worksheet1 = gs.worksheet('FALTANTES en MAESTRO ARTC')\n",
    "    #             worksheet1.clear()\n",
    "    #             # Filter df based on matching ORIN values\n",
    "    #             filtered_df = df[df.ORIN.isin(faltantes_en_lu_articulo.ORIN)]\n",
    "    #             set_with_dataframe(worksheet=worksheet1, dataframe=filtered_df, include_index=False,\n",
    "    #                                include_column_header=True, resize=True)\n",
    "    #         except WorksheetNotFound:\n",
    "    #             # Create new worksheet and add data\n",
    "    #             new_worksheet = gs.add_worksheet(title=\"FALTANTES en MAESTRO ARTC\", rows=check_activos.shape[0],\n",
    "    #                                              cols=check_activos.shape[1])\n",
    "    #             worksheet1 = gs.worksheet('FALTANTES en MAESTRO ARTC')\n",
    "    #             filtered_df = df[df.ORIN.isin(faltantes_en_lu_articulo.ORIN)]\n",
    "    #             set_with_dataframe(worksheet=worksheet1, dataframe=filtered_df, include_index=False,\n",
    "    #                                include_column_header=True, resize=True)\n",
    "    #     else:\n",
    "    #         pass\n",
    "    #\n",
    "\n",
    "\n",
    "    precios=df_no_duplicates.copy()\n",
    "\n",
    "    precios['PVP OFERTA']=precios['PVP OFERTA'].astype(str).str.replace('$',\"\")\n",
    "    precios['PVP OFERTA'] = pd.to_numeric(precios['PVP OFERTA'], errors='coerce')\n",
    "    precios = precios.dropna(subset=['PVP OFERTA'])\n",
    "    precios['PVP OFERTA']=round(precios['PVP OFERTA'].astype(float)).astype(int).astype(str)\n",
    "    a_cargar=info.merge(precios[['ESTADISTICO','PVP OFERTA']].drop_duplicates(), on='ESTADISTICO', how='left')\n",
    "    a_cargar=a_cargar.merge(precios[['ORIN','PVP OFERTA']].drop_duplicates(), on='ORIN', how='left')\n",
    "    a_cargar=a_cargar.merge(df[['ORIN', 'ESTIBA_GUIA_OPERATIVA']], on='ORIN', how='left')\n",
    "\n",
    "\n",
    "    a_cargar['ESTIBA_GUIA_OPERATIVA'] = pd.to_numeric(a_cargar['ESTIBA_GUIA_OPERATIVA'], errors='coerce')\n",
    "    a_cargar['ESTIBA_GUIA_OPERATIVA'].fillna(0, inplace=True)\n",
    "    a_cargar['ESTIBA_GUIA_OPERATIVA'] = a_cargar['ESTIBA_GUIA_OPERATIVA'].astype(int)\n",
    "    a_cargar['ESTIBA_GUIA_OPERATIVA'] = [1 if x != 0 else 0 for x in a_cargar['ESTIBA_GUIA_OPERATIVA']]\n",
    "    a_cargar['ESTIBA_GUIA_OPERATIVA'] = a_cargar['ESTIBA_GUIA_OPERATIVA'].astype(str)\n",
    "\n",
    "    len(info) == a_cargar.shape[0]\n",
    "\n",
    "    # En caso de diferencia\n",
    "\n",
    "    # El scrip lo soluciona\n",
    "    # Deja los q tienen duplicado x un lado , les saca el precio y simplifica\n",
    "\n",
    "    a_cargar['PVP OFERTA']=a_cargar['PVP OFERTA_x'].fillna(a_cargar['PVP OFERTA_y'])\n",
    "    a_cargar=a_cargar[['Fecha Desde', 'Fecha Hasta', 'ESTADISTICO', 'ORIN', 'Nombre Evento',\n",
    "                       'Pronostico de Venta', 'Stock Inicial Promo', 'LOCAL',   'PVP OFERTA', 'ESTIBA_GUIA_OPERATIVA']]\n",
    "\n",
    "    duplis_precios=a_cargar.groupby(['ESTADISTICO','LOCAL'])['PVP OFERTA'].count().reset_index()\n",
    "\n",
    "    duplis_precios.sort_values(by='PVP OFERTA', ascending=False).head()\n",
    "    a_cargar_sin_duplis=a_cargar[~a_cargar.ESTADISTICO.isin(duplis_precios[duplis_precios['PVP OFERTA']>1]['ESTADISTICO'].unique())]\n",
    "    a_cargar_con_dupli=a_cargar[a_cargar.ESTADISTICO.isin(duplis_precios[duplis_precios['PVP OFERTA']>1]['ESTADISTICO'].unique())]\n",
    "    a_cargar_con_dupli.loc[:,'PVP OFERTA']=np.nan\n",
    "    a_cargar_con_dupli=a_cargar_con_dupli.drop_duplicates()\n",
    "    a_cargar_f=pd.concat([a_cargar_con_dupli,a_cargar_sin_duplis])\n",
    "    a_cargar_base=a_cargar_f.copy()\n",
    "    a_cargar_base.rename(columns={'Fecha Desde':'FECHA_DESDE','Fecha Hasta':'FECHA_HASTA', 'Nombre Evento':'EVENTO_ID'}, inplace=True)\n",
    "    a_cargar_base['LOCAL_ACTIVO']=1\n",
    "    a_cargar_base['USER']='LEO'\n",
    "    a_cargar_base['FECHA_DE_CARGA'] = datetime.now().strftime('%Y-%m-%d %H:%M')\n",
    "    a_cargar_base['DETALLE']='carga inicial'\n",
    "    a_cargar_base=a_cargar_base[['FECHA_DESDE', 'FECHA_HASTA','ESTADISTICO', 'EVENTO_ID',\n",
    "                                 'Pronostico de Venta', 'Stock Inicial Promo', 'LOCAL', 'PVP OFERTA','LOCAL_ACTIVO','ESTIBA_GUIA_OPERATIVA',\n",
    "                                 'USER', 'FECHA_DE_CARGA','ORIN','DETALLE']]\n",
    "\n",
    "    ## grabo cantidad de combinaciones cargadas\n",
    "\n",
    "    agrupado_por_item=a_cargar_base.groupby(['ESTADISTICO'])['LOCAL'].count().reset_index().sort_values(by='LOCAL')\n",
    "    agrupado_por_item.rename(columns={'LOCAL':'LOCALES ASIGNADOS'}, inplace=True)\n",
    "\n",
    "    try:\n",
    "\n",
    "        # Try to access the worksheet 'FINALMENTE CARGADO'\n",
    "        try:\n",
    "            worksheetF = gs.worksheet('FINALMENTE CARGADO')\n",
    "            worksheetF.clear()  # Clear the existing data\n",
    "\n",
    "        except gspread.exceptions.WorksheetNotFound:\n",
    "            # Create a new worksheet if it doesn't exist\n",
    "            #print(\"Worksheet 'FINALMENTE CARGADO' does not exist. Creating a new one.\")\n",
    "            worksheetF = gs.add_worksheet(title=\"FINALMENTE CARGADO\", rows=check_activos.shape[0],\n",
    "                                          cols=check_activos.shape[1])\n",
    "\n",
    "        # Now, set the new data into the worksheet\n",
    "        set_with_dataframe(worksheet=worksheetF, dataframe=agrupado_por_item, include_index=False,\n",
    "                           include_column_header=True, resize=True)\n",
    "\n",
    "    except gspread.exceptions.APIError as e:\n",
    "        print(f\"APIError: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\")\n",
    "\n",
    "\n",
    "\n",
    "    # try:\n",
    "    #     #gs.worksheet('FINALMENTE CARGADO')\n",
    "    #     worksheetF = gs.worksheet('FINALMENTE CARGADO')\n",
    "    #     worksheetF.clear()\n",
    "    #     set_with_dataframe(worksheet=worksheetF, dataframe=agrupado_por_item, include_index=False,include_column_header=True, resize=True)\n",
    "    #\n",
    "    # except:\n",
    "    #     new_worksheet = gs.add_worksheet(title=\"FINALMENTE CARGADO\", rows=check_activos.shape[0], cols=check_activos.shape[1])\n",
    "    #     worksheetF = gs.worksheet('FINALMENTE CARGADO')\n",
    "    #     set_with_dataframe(worksheet=worksheetF, dataframe=agrupado_por_item, include_index=False,include_column_header=True, resize=True)\n",
    "\n",
    "    ## Dejo registro de la carga\n",
    "\n",
    "    try:\n",
    "        # Guia Leo\n",
    "        #excel_guia_leo = pd.read_excel('T:\\\\BI\\\\Comercial\\\\ofertas\\\\Leo Cargados.xlsx')\n",
    "        excel_guia_leo = pd.read_excel(excels['excel_detalle_cargados'])\n",
    "\n",
    "        # Eventos\n",
    "        excel_eventos = pd.read_excel(excels['excel_BI'], sheet_name='Eventos')\n",
    "\n",
    "        try:\n",
    "            evento_descripcion = excel_eventos.loc[excel_eventos['evento_id'] == evento_id, 'evento_desc'].iloc[0]\n",
    "\n",
    "        except:\n",
    "            evento_descripcion = excel_eventos.loc[excel_eventos['evento_id'] == int(evento_id), 'evento_desc'].iloc[0]\n",
    "\n",
    "        new_row = {\n",
    "            'Fecha': datetime.today().strftime('%m/%d/%Y'),\n",
    "            'Evento ID': evento_id,\n",
    "            'Descripcion': evento_descripcion,\n",
    "            'Filas': a_cargar_base.shape[0]\n",
    "        }\n",
    "\n",
    "        # Insert the new row at the end\n",
    "        excel_guia_leo.loc[len(excel_guia_leo)] = new_row\n",
    "        excel_guia_leo['Filas'].fillna(0, inplace =True)\n",
    "        excel_guia_leo['Filas'] = excel_guia_leo['Filas'].astype(int)\n",
    "\n",
    "        excel_guia_leo.to_excel(excels['excel_detalle_cargados'], index = False)\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"{gs.title} - File not found. Please check the file paths.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"f{title} - An error occurred:\", e)\n",
    "\n",
    "    ## FT Promos (Excel)\n",
    "\n",
    "    query = '''\n",
    "            SELECT\n",
    "                *\n",
    "            FROM\n",
    "                MSTRDB.DWH.FT_PROMOS\n",
    "            WHERE\n",
    "                EVENTO_ID = {snow_evento_id}\n",
    "            '''\n",
    "\n",
    "    cursor.execute(query.format(snow_evento_id = evento_id))\n",
    "    snow = cursor.fetch_pandas_all()\n",
    "\n",
    "    carga_snow = a_cargar_base.copy()\n",
    "\n",
    "    carga_snow.rename({\n",
    "        'FECHA_DESDE': 'PROM_FECHA_INICIO',\n",
    "        'FECHA_HASTA': 'PROM_FECHA_FIN',\n",
    "        'Fecha_Desde': 'PROM_FECHA_INICIO',\n",
    "        'Fecha_Hasta': 'PROM_FECHA_FIN',\n",
    "        'Evento': 'EVENTO_ID',\n",
    "        'Pronostico de Venta': 'PRONOSTICO_VENTA',\n",
    "        'Pronostico_Venta': 'PRONOSTICO_VENTA',\n",
    "        'Stock Inicial Promo': 'STOCK_INICIAL_PROMO',\n",
    "        'Stock_inicial': 'STOCK_INICIAL_PROMO',\n",
    "        'PVP OFERTA': 'PROM_PVP_OFERTA',\n",
    "        'LOCAL_ACTIVO': 'PROM_LOCAL_ACTIVO',\n",
    "        'Local_activo': 'PROM_LOCAL_ACTIVO',\n",
    "        'Estiba_guia_operativa': 'PROM_ESTIBA',\n",
    "        'ESTIBA_GUIA_OPERATIVA': 'PROM_ESTIBA'\n",
    "    }, axis=1, inplace=True)\n",
    "\n",
    "    query = '''\n",
    "            SELECT\n",
    "                DISTINCT\n",
    "                ARTC_ARTC_ID,\n",
    "                ORIN\n",
    "            FROM\n",
    "                MSTRDB.DWH.LU_ARTC_ARTICULO\n",
    "            '''\n",
    "\n",
    "    cursor.execute(query)\n",
    "    articulos = cursor.fetch_pandas_all()\n",
    "\n",
    "    carga_snow = carga_snow.merge(articulos, on = 'ORIN', how = 'left')\n",
    "\n",
    "    query = '''\n",
    "            SELECT\n",
    "                DISTINCT\n",
    "                GEOG_LOCL_COD AS LOCAL,\n",
    "                GEOG_LOCL_ID\n",
    "            FROM\n",
    "                MSTRDB.DWH.LU_GEOG_LOCAL\n",
    "            '''\n",
    "\n",
    "    cursor.execute(query)\n",
    "    locales = cursor.fetch_pandas_all()\n",
    "\n",
    "    carga_snow = carga_snow.merge(locales, on = 'LOCAL', how = 'left')\n",
    "\n",
    "    carga_snow.drop([\n",
    "        'ESTADISTICO',\n",
    "        'USER',\n",
    "        'FECHA_DE_CARGA',\n",
    "        'ORIN',\n",
    "        'DETALLE',\n",
    "        'LOCAL'\n",
    "    ], axis = 1, inplace = True)\n",
    "\n",
    "    carga_snow['PROM_FECHA_INICIO'] = pd.to_datetime(carga_snow['PROM_FECHA_INICIO'], dayfirst = True).dt.strftime('%Y-%m-%d')\n",
    "    carga_snow['PROM_FECHA_FIN'] = pd.to_datetime(carga_snow['PROM_FECHA_FIN'], dayfirst = True).dt.strftime('%Y-%m-%d')\n",
    "    carga_snow_excel = carga_snow.copy(deep = True)\n",
    "\n",
    "    # Function to format dates\n",
    "    def format_date(date_str):\n",
    "        date_obj = datetime.strptime(date_str, \"%Y-%m-%d\")\n",
    "        return f\"{date_obj.month}/{date_obj.day}/{date_obj.year}\"\n",
    "\n",
    "    # List of columns to apply the formatting\n",
    "    date_columns = ['PROM_FECHA_INICIO', 'PROM_FECHA_FIN']\n",
    "\n",
    "    # Apply the formatting function to each column\n",
    "    for col in date_columns:\n",
    "        carga_snow_excel[col] = carga_snow_excel[col].apply(format_date)\n",
    "\n",
    "    query = '''\n",
    "            SELECT\n",
    "                GEOG_LOCL_ID,\n",
    "                GEOG_LOCL_COD\n",
    "            FROM\n",
    "                MSTRDB.DWH.LU_GEOG_LOCAL\n",
    "            '''\n",
    "\n",
    "    cursor.execute(query)\n",
    "    local_snow = cursor.fetch_pandas_all()\n",
    "\n",
    "    carga_snow_excel = carga_snow_excel.merge(local_snow, on = 'GEOG_LOCL_ID', how = 'inner')\n",
    "\n",
    "    query = '''\n",
    "            SELECT\n",
    "                ARTC_ARTC_ID,\n",
    "                ARTC_ARTC_COD\n",
    "            FROM\n",
    "                MSTRDB.DWH.LU_ARTC_ARTICULO\n",
    "            '''\n",
    "\n",
    "    cursor.execute(query)\n",
    "    articulo_snow = cursor.fetch_pandas_all()\n",
    "\n",
    "    carga_snow_excel = carga_snow_excel.merge(articulo_snow, on = 'ARTC_ARTC_ID', how = 'inner')\n",
    "\n",
    "    carga_snow_excel = carga_snow_excel[[\n",
    "        'PROM_FECHA_INICIO',\n",
    "        'PROM_FECHA_FIN',\n",
    "        'ARTC_ARTC_COD',\n",
    "        'EVENTO_ID',\n",
    "        'PRONOSTICO_VENTA',\n",
    "        'STOCK_INICIAL_PROMO',\n",
    "        'GEOG_LOCL_COD',\n",
    "        'PROM_PVP_OFERTA',\n",
    "        'PROM_LOCAL_ACTIVO',\n",
    "        'PROM_ESTIBA'\n",
    "    ]]\n",
    "\n",
    "    path = paths['files_para_BI']\n",
    "    file_name = f\"{evento_id} - {gs.title.split('(')[0]}, {fini.replace('/', '.')} al {ffin.replace('/', '.')} - {datetime.today().strftime('%Y-%m-%d')}.xlsx\"\n",
    "\n",
    "    path_file_name = os.path.join(path, file_name)\n",
    "\n",
    "    carga_snow_excel.to_excel(f\"{path_file_name}\", index = False)\n",
    "\n",
    "    hoy = datetime.today().date().strftime('%Y-%m-%d')\n",
    "\n",
    "    path = paths['respaldo_BI']\n",
    "    file_name = f\"{nombre_evento.replace('/', '.')} - {evento_id} - {str(fini).replace('/', '.')} a {str(ffin).replace('/', '.')}.csv\"\n",
    "    path_file_name = os.path.join(path, file_name)\n",
    "\n",
    "    try:\n",
    "        carga_snow_excel.to_csv(path_file_name,index=False)\n",
    "\n",
    "    except:\n",
    "        carga_snow_excel.to_csv(file_name, index=False) # Si no puede guardar en la carpeta compartida, guarda en nuestro directorio local\n",
    "        print(f\"{gs.title}, {fini} al {ffin} -Guardado en PC\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "05. OFERTATA ENERO 04 (2412) ARMADO\n",
      "Estados: (4)\n",
      "\n",
      "Termina descargar_promos_url\n"
     ]
    }
   ],
   "source": [
    "# Parte 2. Llamo la funcion para cada url y para las sheets que corresponda segun si es frescos o no\n",
    "\n",
    "with open('URLS.txt', 'r') as file:\n",
    "# Skip the first row\n",
    "    next(file)\n",
    "\n",
    "    # Iterate over the remaining rows\n",
    "    for row in file:\n",
    "\n",
    "        # Remove any leading/trailing whitespace characters\n",
    "        url = row.strip()\n",
    "\n",
    "        codigo_url = url.split('/')[-2]\n",
    "        gs = gc.open_by_key(codigo_url)\n",
    "\n",
    "        # Frescos: iteramos por cada worksheet que empiece con un numero\n",
    "        if 'FRESCOS' in gs.title.upper():\n",
    "            for worksheet in gs.worksheets():\n",
    "                print('')\n",
    "                if re.match(r'^\\d', worksheet.title):\n",
    "                    print(gs.title, worksheet.title)\n",
    "                    descargar_promos_url(cursor, url, worksheet)\n",
    "\n",
    "        # No Frescos: vamos a la worksheet Armado o Listado\n",
    "        else:\n",
    "            print('')\n",
    "            try:\n",
    "                worksheetL = gs.worksheet('Listado')\n",
    "            except gspread.exceptions.WorksheetNotFound:\n",
    "                try:\n",
    "                    worksheetL = gs.worksheet('ARMADO')\n",
    "                except gspread.exceptions.WorksheetNotFound:\n",
    "                    print(f\"{gs.title} - Listado and Armado worksheets not found\")\n",
    "            print(gs.title, worksheetL.title)\n",
    "            descargar_promos_url(cursor, url, worksheetL)\n",
    "\n",
    "print('')\n",
    "print('Termina descargar_promos_url')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Fin"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
